---
layout:     post   				    # 使用的布局
title:      34.0 基于Attention-CNN的语义匹配模型				# 标题 
date:       2018-09-25 				# 时间
author:     子颢 						# 作者
catalog: true 						# 是否归档
tags:								#标签
    - 深度学习
    - ABCNN
    - QQ match
    - 语义匹配
---

# 算法原理

前面我们说到，DSSM与QQ-match模型非常类似，解决的问题领域也差不多，只不过QQ-match的输入是QQ pair（一对一），判断用户query与doc是否match。而DSSM的输入是一对多，即一个query，多篇doc，相当于直接做语义相似度检索。但是在实际工作中，DSSM的操作比较复杂，需要将输入打成固定的一query多doc的格式，但是搜索引擎往往需要在海量数据中进行语义检索，所以标注数据的获取难道较大，而且最终模型的效果也并不稳定，甚至难以收敛。所以QQ-match实际上更为常用，尤其在智能问答、搜索领域，通常的做法是先通过全文检索的方式做初筛得到一批候选集，然后再通过deep QQ match模型做语义匹配进行reranking和精排。<br>
下面会为大家介绍三种常用的deep QQ match模型：BCNN、ABCNN和Hybrid CNN。

## BCNN

BCNN（Bilinear CNN），BCNN的原理非常简单，直接在QQ pair上分别进行CNN卷积操作，得到每一个query的语义representation，然后输入逻辑回归模型或MLP做二分类。
![ABCNN](/img/ABCNN-01.png)
注意：
- 对于卷积层，使用的是wide convolution，也就是我们之前在卷积神经网络课程中讲到的转置卷积（反卷积）。
- 对于pooling层，由于使用了两层CNN，最后一个pooling层使用all-ap，中间卷积层的pooling层使用w-ap。all-ap：即将卷积之后的结果在句长维度上进行Average Pooling，也就是我们之前在卷积神经网络课程中讲到的global pooling；w-ap：即一般的卷积方式，使用滑动窗口的形式，以窗口宽度w对卷积输出进行Average Pooling。

## ABCNN

ABCNN（Attention-based CNN），即在BCNN的基础上加入了Attention机制，让模型自己学到query与query之间的这种隐式的对其关系，其他部分和BCNN完全一致。
![ABCNN](/img/ABCNN-02.png)
1. 首先计算attention矩阵A，其每个元素Aij代表query1中第i个单词对query2中第j个单词的match score，match score可以使用Euclidean距离[1/(1 + Math.abs(x-y))]计算也可以使用dot product计算[x * y]。
2. 分别计算两个query的attention feature map。如图所示，使用两个参数矩阵W0, W1分别与A和A的转置相乘（即矩阵变换）即可获得与原本feature尺寸相同的特征图。这里W0和W1可以是同一个参数矩阵，即权值共享。
3. 将上一步得到的attention feature map与query embedding map做纵向concat以分别作为卷积channel，然后进行卷积操作。

上一种ABCNN直接在输入层加Attention，目的是为了得到原始query与query之间的隐式对其关系。其实还可以在第一层卷积层的输出结果上加入Attention，以得到query的隐式representation之间的的隐式对其关系。
![ABCNN](/img/ABCNN-03.png)
1. Attention矩阵的计算方法与上述相同。
2. 还需要分别为两个query计算两个Attention权重向量，如上图中的两个虚线部分，这里直接按行列相加，分别得到col-wise Attention（其每个元素分别代表query1中每个字和query2整体的对其权重）和row-wise Attention（其每个元素分别代表query2中每个字和query1整体的对其权重）。
3. 2中得到的两个Attention权重向量中的每个元素分别代表了相应单词在做Average Pooling时的权重。即该模型修改了pooling方法，不再是简单的Average Pooling，而是乘以计算出的Attention权重向量后再计算。公式如下：

相比第一种ABCNN而言，第二种ABCNN是通过对卷积输出结果进行赋权，以改善pooling的结果，使获得的高层次抽象特征中不同词按照不同的权重进行叠加。所以还可以将以上两个模型融合在一起得到一个新的模型，新的模型兼备以上两种模型的优点。
![ABCNN](/img/ABCNN-04.png)

代码地址 <a href="https://github.com/galsang/ABCNN.git" target="_blank">https://github.com/galsang/ABCNN.git</a>

## Hybrid CNN

Hybrid CNN，混合的Attention CNN模型，它和ABCNN的第一种形式有些类似，ABCNN是将attention feature map与query embedding map做纵向concat作为卷积channel，然后进行卷积操作；Hybrid CNN则是将Attention Matrix单独提出来进行卷积操作。其模型架构如下所示：
![ABCNN](/img/ABCNN-05.png)
1. Attention矩阵的计算方法与上述相同。
2. 将两个query与Attention Matrix分别进行卷积操作，相当于将ABCNN的三通道分别用不同的卷积核进行特征提取，而不是将三通道concat后用一个卷积核提取。
3. 将2步得到的特征表达做concat，送入全连接神经网络进行二分类。

Hybrid CNN网络结构简单清晰，与ABCNN异曲同工，只不过在对Attention Matrix的卷积操作上有所不同，并且最后通过全连接神经网络学习出query1、query2以及Attention representation两两间的错综复杂的关系。

# 社群

- 微信公众号
	![562929489](/img/wxgzh_ewm.png)